import torch
import torch.nn as nn
from torch.utils.data import Dataset
from torch.utils.data import DataLoader
from sklearn.metrics import roc_auc_score
from sklearn.metrics import average_precision_score

import numpy as np



ITEMID_SET = [225280, 2, 3, 4, 5, 8550, 2060, 227341, 227342, 227343, 227344, 227345, 227346, 227348, 227349, 227350, 227351, 227352, 227353, 26, 227355, 227356, 225309, 225310, 227358, 225312, 225313, 227360, 227357, 227359, 227365, 227366, 227367, 227368, 227369, 822, 227364, 227363, 823, 2092, 2095, 224704, 50, 51, 52, 55, 2103, 58, 59, 60, 826, 6206, 6207, 6208, 62, 64, 66, 67, 69, 70, 71, 63, 65, 74, 75, 225358, 79, 87, 89, 90, 4187, 4188, 92, 227418, 227419, 2139, 2141, 2146, 4196, 227429, 4197, 4199, 4200, 4201, 4202, 4203, 835, 225389, 227439, 227440, 113, 227442, 227443, 116, 227445, 114, 227446, 227444, 2161, 227441, 2166, 225398, 227447, 25, 227454, 227456, 227457, 227455, 131, 227460, 227461, 227462, 227463, 227464, 227465, 227466, 227467, 227468, 141, 227470, 227471, 143, 227469, 2194, 146, 142, 842, 29, 844, 153, 2201, 2203, 155, 8362, 441, 8363, 173, 8368, 176, 8370, 8371, 178, 181, 184, 185, 186, 227516, 189, 190, 445, 192, 8385, 194, 195, 8387, 198, 855, 448, 227537, 227538, 211, 856, 227543, 2265, 218, 219, 227546, 221, 227549, 227547, 224, 226, 227565, 227566, 2287, 8440, 8441, 227579, 227580, 8444, 227582, 8446, 8448, 8449, 227586, 8445, 2305, 227581, 227602, 227604, 1684, 1685, 227614, 8483, 8484, 8487, 2344, 8491, 227627, 227628, 8495, 227632, 8496, 227634, 8498, 308, 8501, 8502, 8503, 8504, 61, 8506, 8507, 8508, 309, 8505, 8511, 8517, 8518, 2373, 2376, 1694, 225612, 2380, 8527, 8531, 8532, 6484, 8533, 225624, 225625, 8537, 225628, 225634, 8547, 8548, 8549, 225638, 8551, 225640, 227688, 8553, 8554, 8552, 225639, 225642, 225641, 225643, 8555, 3736, 225651, 3737, 225664, 225667, 225668, 225671, 225672, 225674, 227723, 225677, 227727, 224702, 227731, 225684, 3744, 225690, 225692, 225693, 225695, 3746, 417, 225698, 421, 422, 5783, 6573, 430, 431, 434, 435, 436, 2482, 437, 439, 440, 2487, 225722, 443, 444, 442, 225724, 223679, 225723, 449, 450, 225729, 225730, 2498, 454, 455, 456, 225732, 225736, 459, 6605, 470, 471, 472, 473, 227801, 227802, 474, 227807, 480, 227809, 227810, 481, 484, 485, 227812, 483, 482, 225768, 490, 491, 492, 493, 494, 495, 496, 497, 498, 225771, 500, 4576, 2544, 504, 505, 506, 507, 3765, 512, 513, 3766, 517, 223751, 223752, 223753, 519, 521, 520, 225806, 225807, 223761, 223762, 530, 532, 225813, 223763, 535, 223768, 223769, 223770, 223767, 223772, 223773, 223765, 543, 223771, 2589, 6701, 6702, 223791, 223805, 223806, 578, 223811, 580, 581, 223810, 223814, 583, 223817, 223818, 223819, 588, 223820, 223821, 589, 586, 595, 223830, 601, 223834, 223835, 602, 606, 607, 611, 614, 615, 223848, 223849, 618, 619, 6635, 624, 625, 626, 627, 2673, 631, 223872, 223873, 223874, 223875, 223876, 225637, 646, 225636, 651, 652, 4753, 662, 664, 132, 666, 223900, 223901, 670, 671, 672, 669, 676, 677, 678, 679, 681, 682, 683, 684, 686, 225980, 225981, 701, 225989, 225990, 227686, 719, 721, 723, 724, 725, 223958, 727, 223960, 729, 223962, 223963, 730, 733, 734, 226062, 228097, 741, 742, 228102, 762, 763, 228096, 769, 770, 228099, 228100, 773, 228098, 772, 776, 777, 778, 779, 780, 781, 228101, 775, 784, 785, 786, 787, 788, 789, 226063, 791, 792, 793, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805, 806, 807, 6947, 228137, 809, 811, 226092, 813, 814, 815, 816, 817, 818, 226094, 226096, 821, 224054, 224055, 224056, 224057, 224058, 827, 224059, 829, 828, 824, 825, 833, 834, 226107, 226113, 837, 226109, 226114, 226115, 226117, 226118, 836, 226119, 838, 226121, 226120, 848, 849, 850, 851, 852, 853, 226129, 226135, 226136, 226137, 226138, 226139, 860, 861, 857, 858, 859, 226140, 2965, 2921, 226169, 226170, 226179, 226180, 228229, 228230, 228231, 228232, 226185, 228234, 228233, 228236, 220045, 220046, 220047, 224144, 220050, 220051, 220052, 224149, 224150, 2967, 920, 220056, 220058, 220059, 220060, 220061, 224152, 220063, 224151, 224161, 224162, 220066, 2973, 224167, 224168, 220072, 220074, 220073, 224169, 224172, 224178, 226228, 220088, 224191, 224192, 7107, 226108, 228299, 226253, 226110, 228305, 220120, 226112, 228318, 226272, 228334, 7151, 228336, 228337, 226116, 228335, 1010, 228343, 228344, 228345, 228346, 3067, 226122, 228368, 228369, 220179, 220180, 220181, 228374, 228375, 228376, 226329, 228373, 228381, 228382, 220194, 228394, 228395, 228396, 220210, 3122, 224309, 224310, 224311, 228409, 228410, 228411, 228412, 225725, 220224, 224322, 220227, 220228, 227354, 225726, 224328, 224329, 224330, 220235, 224332, 226381, 224331, 226134, 1104, 1106, 3168, 1121, 1126, 1127, 224919, 224359, 227362, 224920, 224366, 224367, 228177, 220274, 224921, 3483, 220277, 228178, 224922, 228179, 3485, 228180, 3486, 220292, 220293, 227775, 1162, 228182, 228184, 224404, 224406, 228185, 224409, 224410, 224411, 226457, 224929, 7325, 224417, 224418, 1187, 224420, 224421, 224422, 1191, 224419, 226473, 220339, 225342, 1211, 3497, 224447, 226499, 1226, 226512, 3288, 3294, 226531, 226534, 226536, 226537, 226540, 226543, 226544, 3312, 3313, 3311, 3315, 3314, 3317, 3316, 3320, 3321, 3322, 3323, 3324, 3325, 3318, 3319, 1286, 3337, 3345, 224954, 224955, 1313, 224956, 1317, 224957, 7469, 224562, 224563, 224564, 1331, 1336, 3386, 1340, 3400, 227813, 1352, 3404, 1360, 1370, 220507, 3420, 3421, 3422, 224154, 224153, 1380, 3429, 1383, 3436, 3437, 1390, 1391, 1397, 224631, 3447, 1401, 3450, 3451, 1402, 224639, 1407, 224641, 220545, 220546, 3460, 3459, 224643, 3457, 1411, 224646, 224645, 224647, 224652, 224654, 1424, 226707, 224661, 224662, 224663, 224665, 224666, 224667, 224668, 224669, 224670, 224671, 224672, 224673, 224674, 224675, 220580, 220581, 224676, 3495, 224677, 3494, 226730, 220587, 224684, 224685, 224686, 224687, 224688, 224689, 224690, 3503, 3502, 5556, 224691, 224695, 224696, 224697, 220602, 220603, 224700, 224701, 223764, 5565, 223766, 224705, 224706, 3523, 224707, 224709, 1524, 220615, 224711, 224710, 220621, 1486, 220624, 220632, 1526, 224728, 220635, 3547, 1497, 1520, 224738, 3555, 220644, 220645, 3556, 1506, 1513, 220650, 224746, 3564, 224747, 224750, 224751, 224752, 1521, 1522, 1523, 224753, 1525, 224755, 224754, 1528, 1529, 1530, 1531, 1532, 1533, 1534, 1535, 1536, 3581, 1538, 3580, 3582, 1540, 1542, 1527, 1541, 1539, 1546, 3599, 3603, 3609, 1565, 226846, 3614, 1567, 1566, 1575, 1578, 5683, 1537, 226871, 226873, 3641, 224828, 1596, 220734, 1598, 1599, 3647, 1600, 220739, 3652, 224837, 3654, 3655, 224839, 226889, 224842, 224838, 226892, 224845, 224846, 2355, 3664, 1616, 5708, 1621, 224855, 1624, 224856, 220765, 5726, 5727, 3681, 1633, 1639, 3688, 3689, 3583, 3692, 3693, 1650, 1654, 1659, 1662, 224895, 224896, 224897, 224898, 224899, 224900, 1671, 1673, 3723, 1677, 3728, 224916, 224917, 3734, 3735, 224918, 3732, 3738, 224923, 224924, 224925, 3742, 1695, 1696, 1697, 3745, 224926, 3740, 3747, 224927, 224928, 1704, 1705, 3754, 1707, 1706, 3750, 3759, 3761, 5813, 5814, 5815, 3768, 5817, 5818, 5819, 5820, 3771, 3770, 224951, 224952, 5816, 224953, 3779, 3780, 224965, 224966, 224967, 224968, 3785, 3784, 224969, 224970, 3789, 1738, 3791, 3792, 224971, 3799, 3801, 3802, 3803, 3808, 3809, 3810, 1761, 7920, 3829, 3830, 3831, 3832, 3834, 3835, 227066, 3837, 3838, 3839, 3836, 227073, 6870, 1803, 8500, 479, 768, 225067, 225070, 225074, 225076, 225078, 225082, 227130, 225085, 225086, 225087, 1853, 1856, 225091, 225092, 225094, 225103, 225105, 225106, 225108, 225110, 225112, 225113, 225117, 225118, 225122, 1891, 225124, 225126, 225129, 1898, 225133, 227187, 225142, 1922, 1923, 1924, 1925, 1926, 1927, 1928, 1929, 8074, 1930, 1931, 225175, 6041, 225183, 225184, 225185, 225187, 225188, 225189, 225191, 225192, 227242, 227243, 225206, 225209, 225210, 6073, 3651, 1987, 225228, 2000, 227287, 227292, 227293, 8159, 810, 2033]

def load_model(model, model_path):
    state = torch.load(model_path)
    model.load_state_dict(state['model'])

def save_model(model, model_path):
    state = {
        'model': model.state_dict()
    }
    torch.save(state, model_path)

class ICUDataset(Dataset):
    def __init__(self, data_path_X, data_path_y, max_len, item_id_set) -> None:
        super(ICUDataset, self).__init__()

        self.data = open(data_path_X, "rt").readlines()
        self.max_len = max_len
        self.labels = np.load(data_path_y)
        
        self.item_id_dict = dict()
        self.item_id_dict[0] = 0
        item_id_set = list(item_id_set)
        for i in range(len(item_id_set)):
            self.item_id_dict[item_id_set[i]] = i+1

    def __len__(self) -> int:
        return len(self.data)
    
    def __getitem__(self, index):
        line = self.data[index].strip()
        tokens = line.split(" ")[:self.max_len]

        if tokens == [""]: tokens = ["0:0:0"]
        if len(tokens) < self.max_len: 
            tokens += ["0:0:0" for _ in range(self.max_len - len(tokens))]

        time = np.array(list(map(lambda x: x.split(":")[0], tokens)), dtype=np.double)
        itemid = np.array(list(map(lambda x: self.item_id_dict[int(x.split(":")[1])], tokens)), dtype=int)
        valuenum = np.array(list(map(lambda x: x.split(":")[2], tokens)), dtype=np.double)
        label = torch.zeros(2)
        label[int(self.labels[index])] = 1

        return {'label': label,
                'time': time,
                'itemid': itemid,
                'valuenum': valuenum}


class GRU(nn.Module):
    def __init__(self, d_itemid, hidden_dim, n_layers=1, n_class=2):
        super(GRU, self).__init__()    
        self.itemid_embedding = nn.Embedding(len(ITEMID_SET) + 1, d_itemid)
        self.gru = nn.GRU(1 + d_itemid + 1, 2 * hidden_dim, n_layers, batch_first=True, ) # charttime:itemid:value
        self.outer = nn.Linear(2 * hidden_dim, hidden_dim)
        self.classifier = nn.Linear(hidden_dim, n_class)

    def forward(self, x):
        x_time = x['time']
        batch_size = x_time.shape[0]
        x_itemid = x['itemid']
        x_val = x['valuenum']
        emb_itemid = self.itemid_embedding(x_itemid)

        x_time = x_time.reshape((batch_size,100,1))
        x_val = x_val.reshape((batch_size,100,1))
        input = torch.cat((x_time, emb_itemid, x_val), dim=2).float()

        out, _ = self.gru(input)
        out = self.outer(out[:,-1,:])
        out = self.classifier(out)
        return out
    
def predict(model, dataset, device = 'cuda'):
    print(f'current device setting: {device}')
    model.eval()
    
    test_dataset = ICUDataset(dataset[0], dataset[1], 100, ITEMID_SET)
    test_data_loader = DataLoader(test_dataset, batch_size=10,  shuffle=False) 
    total_predict = torch.tensor([]).to(device)

    for batch in test_data_loader:
        # data bach
        data = {
            'time': batch['time'].to(device),
            'itemid': batch['itemid'].to(device),
            'valuenum': batch['valuenum'].to(device),
        }
        
        # prediction
        pred = model(data)
        
        # for logging
        pred = torch.sigmoid(pred)
        tmp = torch.Tensor(pred)
        tmp[:,0] = tmp[:,0]*0.225
        pred = tmp

        pred_y = torch.max(pred,1)[1].to(device)
        total_predict = torch.cat((total_predict, pred_y))
        
    return total_predict

if __name__ == "__main__":
    #device = 'cuda'
    #print("Default Setting : CUDA, If there is no cuda, then use the gru-cpu.pth as model parameter")
    #print("There is CPU version code in test_rnn.py(which is wrapped by #)")
    #model = GRU(256, 64).to(device)
    #load_model(model, './gru.pth')

    device = 'cpu'
    print("Current Setting : CPU")
    model = GRU(256, 64).to(device)
    load_model(model, './gru-cpu.pth')


    # doesn't use label in here
    # I've use label during training time
    train_pred = predict(model, dataset=['./X_train_rnn.npy', './y_train.npy'], device=device)
    train_pred = np.array(train_pred.cpu())
    train_label = np.load('./y_train.npy')

    # doesn't use label in here
    # I've use label during training time
    test_pred = predict(model, dataset=['./X_test_rnn.npy', './y_test.npy'], device=device)
    test_pred = np.array(test_pred.cpu())
    test_label = np.load('./y_test.npy')


    fout = open("./20233477_rnn.txt", 'wt')
    fout.write(f'20233477\n')
    fout.write(f'{roc_auc_score(train_label, train_pred)}\n')
    fout.write(f'{average_precision_score(train_label, train_pred)}\n')
    fout.write(f'{roc_auc_score(test_label, test_pred)}\n')
    fout.write(f'{average_precision_score(test_label, test_pred)}\n')
